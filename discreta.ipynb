{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7dc6e88-3d56-48a1-a77e-9cabb57805a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e5ee3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "    y=df.quality\n",
    "    X=df.drop('quality',1)\n",
    "    X = min_max_scaler.fit_transform(X)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45c4b23d-1f27-4596-9b1c-eed4d8cbd32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "#CREATE AND MERGE DATAFRAMES\n",
    "dfRed=pd.read_csv(\"winequality-red.csv\", sep=\";\")\n",
    "dfWhite=pd.read_csv(\"winequality-white.csv\", sep=\";\")\n",
    "\n",
    "\n",
    "dfRed['Type']=\"Red\"\n",
    "dfWhite['Type']=\"White\"\n",
    "df=pd.concat([dfRed,dfWhite])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d419aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81e1035f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\walli\\AppData\\Local\\Temp/ipykernel_15656/3104105429.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X=df.drop('quality',1)\n"
     ]
    }
   ],
   "source": [
    "df['Type']=labelencoder.fit_transform(df['Type'])\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "dfAux=df.copy()\n",
    "df.loc[dfAux['quality'] < 6, 'quality'] = \"Baixa\"\n",
    "df.loc[dfAux['quality'] == 6, 'quality'] = \"Mitjana\"\n",
    "df.loc[dfAux['quality'] > 6, 'quality'] = \"Alta\"\n",
    "del dfAux\n",
    "\n",
    "X, y = load_data(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5ea832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6497 entries, 3736 to 730\n",
      "Data columns (total 13 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         6497 non-null   float64\n",
      " 1   volatile acidity      6497 non-null   float64\n",
      " 2   citric acid           6497 non-null   float64\n",
      " 3   residual sugar        6497 non-null   float64\n",
      " 4   chlorides             6497 non-null   float64\n",
      " 5   free sulfur dioxide   6497 non-null   float64\n",
      " 6   total sulfur dioxide  6497 non-null   float64\n",
      " 7   density               6497 non-null   float64\n",
      " 8   pH                    6497 non-null   float64\n",
      " 9   sulphates             6497 non-null   float64\n",
      " 10  alcohol               6497 non-null   float64\n",
      " 11  quality               6497 non-null   object \n",
      " 12  Type                  6497 non-null   int32  \n",
      "dtypes: float64(11), int32(1), object(1)\n",
      "memory usage: 685.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d4978fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Alta       0.54      0.33      0.41       288\n",
      "       Baixa       0.63      0.62      0.62       613\n",
      "     Mitjana       0.54      0.62      0.58       724\n",
      "\n",
      "    accuracy                           0.57      1625\n",
      "   macro avg       0.57      0.52      0.54      1625\n",
      "weighted avg       0.57      0.57      0.57      1625\n",
      "\n",
      "training_accuracy: 0.5847701149425287\n",
      "testing_accuracy: 0.571076923076923\n"
     ]
    }
   ],
   "source": [
    "#LOGISTIC REGRESSION\n",
    "\n",
    "logreg = LogisticRegression(max_iter=5000)\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "y_predict = logreg.predict(X_test)\n",
    "\n",
    "# dfLogReg=pd.DataFrame()\n",
    "# dfLogReg['Real'] = y_test\n",
    "# dfLogReg['Prediction']=y_predict\n",
    "# print(dfLogReg.head())\n",
    "\n",
    "print(classification_report(y_test, y_predict))\n",
    "\n",
    "training_accuracy = logreg.score(X_train,y_train)\n",
    "print('training_accuracy:',training_accuracy)\n",
    "\n",
    "testing_accuracy = logreg.score(X_test,y_test)\n",
    "print('testing_accuracy:',testing_accuracy)\n",
    "\n",
    "#TODO: Perqu√® dona aquells warnings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1292fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Alta       0.39      0.64      0.48       288\n",
      "       Baixa       0.48      0.89      0.62       613\n",
      "     Mitjana       0.00      0.00      0.00       724\n",
      "\n",
      "    accuracy                           0.45      1625\n",
      "   macro avg       0.29      0.51      0.37      1625\n",
      "weighted avg       0.25      0.45      0.32      1625\n",
      "\n",
      "training_accuracy: 0.46798029556650245\n",
      "testing_accuracy: 0.45046153846153847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\walli\\miniconda3\\envs\\ml-environment\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\walli\\miniconda3\\envs\\ml-environment\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\walli\\miniconda3\\envs\\ml-environment\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#PERCEPTRON\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "y_predict=perceptron.predict(X_test)\n",
    "\n",
    "# dfPerceptron=pd.DataFrame()\n",
    "# dfPerceptron['Real'] = y_test\n",
    "# dfPerceptron['Prediction']=y_predict\n",
    "# print(dfPerceptron.head)\n",
    "\n",
    "print(classification_report(y_test, y_predict))\n",
    "\n",
    "training_accuracy = perceptron.score(X_train,y_train)\n",
    "print('training_accuracy:',training_accuracy)\n",
    "\n",
    "testing_accuracy = perceptron.score(X_test,y_test)\n",
    "print('testing_accuracy:',testing_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d1f45ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Alta       0.75      0.67      0.71       288\n",
      "       Baixa       0.77      0.74      0.76       613\n",
      "     Mitjana       0.69      0.74      0.72       724\n",
      "\n",
      "    accuracy                           0.73      1625\n",
      "   macro avg       0.74      0.72      0.73      1625\n",
      "weighted avg       0.73      0.73      0.73      1625\n",
      "\n",
      "training_accuracy: 0.9909688013136289\n",
      "testing_accuracy: 0.7310769230769231\n"
     ]
    }
   ],
   "source": [
    "#RANDOM FOREST\n",
    "rf = RandomForestClassifier(max_depth=14)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_predict=rf.predict(X_test)\n",
    "\n",
    "# dfRF=pd.DataFrame()\n",
    "# dfRF['Real'] = y_test\n",
    "# dfRF['Prediction']=y_predict\n",
    "# print(dfRF.head)\n",
    "\n",
    "print(classification_report(y_test, y_predict))\n",
    "\n",
    "training_accuracy = rf.score(X_train,y_train)\n",
    "print('training_accuracy:',training_accuracy)\n",
    "\n",
    "testing_accuracy = rf.score(X_test,y_test)\n",
    "print('testing_accuracy:',testing_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
